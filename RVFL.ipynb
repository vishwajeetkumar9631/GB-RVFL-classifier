{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat(r'C:\\Users\\vishw\\Documents\\karafeature\\MM08.mat')\n",
    "lab =loadmat(r'C:\\Users\\vishw\\Documents\\karafeature\\LabelMM081.mat')\n",
    "# Extract features and labels\n",
    "features = data['all_features']\n",
    "labels = lab['all_labels'].flatten()\n",
    "# labels = lab['all_labels'].flatten()  # Flatten if labels are in (8184, 1) shape\n",
    "# If labels start from 1, adjust them to start from 0\n",
    "if np.min(labels) == 1:\n",
    "    labels -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optimized activation functions using NumPy\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772\n",
    "    scale = 1.0507009873554805\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def hardlim(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tribas(x):\n",
    "    return np.maximum(1 - np.abs(x), 0)\n",
    "\n",
    "def radbas(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "# Mapping activation functions to their respective functions\n",
    "activation_functions = {\n",
    "    1: selu,\n",
    "    2: relu,\n",
    "    3: sigmoid,\n",
    "    4: np.sin,\n",
    "    5: hardlim,\n",
    "    6: tribas,\n",
    "    7: radbas,\n",
    "    8: np.sign,\n",
    "    9: leaky_relu,\n",
    "    10: np.tanh\n",
    "}\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot(x, n_class):\n",
    "    y = np.zeros((len(x), n_class))\n",
    "    for i in range(n_class):\n",
    "        y[x == i, i] = 1\n",
    "    return y\n",
    "\n",
    "# Evaluation metrics\n",
    "def Evaluate(ACTUAL, PREDICTED):\n",
    "    p = np.sum(ACTUAL == 1)\n",
    "    n = np.sum(ACTUAL == 0)\n",
    "    \n",
    "    tp = np.sum((ACTUAL == 1) & (PREDICTED == 1))\n",
    "    tn = np.sum((ACTUAL == 0) & (PREDICTED == 0))\n",
    "    fp = np.sum((ACTUAL == 0) & (PREDICTED == 1))\n",
    "    fn = np.sum((ACTUAL == 1) & (PREDICTED == 0))\n",
    "    \n",
    "    accuracy = 100 * (tp + tn) / (p + n)\n",
    "    sensitivity = 100 * tp / p if p != 0 else 0\n",
    "    specificity = 100 * tn / n if n != 0 else 0\n",
    "    precision = 100 * tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = sensitivity\n",
    "    f_measure = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    gmean = 100 * np.sqrt((tp / p) * (tn / n)) if p != 0 and n != 0 else 0\n",
    "    \n",
    "    return [accuracy, sensitivity, specificity, precision, recall, f_measure, gmean, tp, tn, fp, fn]\n",
    "\n",
    "def RVFL_train(train_data, test_data, C, N, activation):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Split training data\n",
    "    trainX = train_data[:, :-1]\n",
    "    trainY = train_data[:, -1]\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    nclass = 2\n",
    "    trainY_encoded = one_hot(trainY.astype(int), nclass)\n",
    "    \n",
    "    # Initialize random weights and biases\n",
    "    Nsample, Nfea = trainX.shape\n",
    "    W = np.random.uniform(-1, 1, (Nfea, N))\n",
    "    b = 0.1 * np.random.rand(1, N)\n",
    "    \n",
    "    # Compute hidden layer output for training data\n",
    "    X1 = np.dot(trainX, W) + b\n",
    "    activation_func = activation_functions.get(activation, relu)\n",
    "    X1 = activation_func(X1)\n",
    "    \n",
    "    # Augment the input with hidden layer output and bias term\n",
    "    X = np.hstack((trainX, X1, np.ones((Nsample, 1))))\n",
    "    \n",
    "    # Compute output weights (beta) using regularized least squares\n",
    "    if X.shape[1] < Nsample:\n",
    "        beta = np.linalg.inv(np.eye(X.shape[1]) / C + X.T @ X) @ X.T @ trainY_encoded\n",
    "    else:\n",
    "        beta = X.T @ np.linalg.inv(np.eye(X.shape[0]) / C + X @ X.T) @ trainY_encoded\n",
    "\n",
    "    # Test phase\n",
    "    testX = test_data[:, :-1]\n",
    "    testY = test_data[:, -1]\n",
    "    \n",
    "    X1_test = np.dot(testX, W) + b\n",
    "    X1_test = activation_func(X1_test)\n",
    "    \n",
    "    # Augment test data\n",
    "    X_test = np.hstack((testX, X1_test, np.ones((testX.shape[0], 1))))\n",
    "    \n",
    "    # Calculate scores and classify\n",
    "    raw_scores = X_test @ beta\n",
    "    predicted_labels = np.argmax(raw_scores, axis=1)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    eval_metrics = Evaluate(testY, predicted_labels)\n",
    "    end = time.time()\n",
    "    \n",
    "    return eval_metrics, end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (Accuracy, Sensitivity, Specificity, Precision, Recall, F1 Score, G-Mean):\n",
      "[97.54385964912281, 96.32352941176471, 98.65771812080537, 98.49624060150376, 96.32352941176471, 97.39776951672864, 97.48363766862103, 131, 147, 2, 5]\n",
      "Training and Testing Time: 4.319603681564331 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "dataset = np.hstack((features, labels.reshape(-1, 1)))\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[:, :-1] = scaler.fit_transform(train_data[:, :-1])\n",
    "test_data[:, :-1] = scaler.transform(test_data[:, :-1])\n",
    "\n",
    "C = 0.01       # Regularization parameter\n",
    "N = 100        # Number of random hidden neurons\n",
    "activation = 2  # Activation function index (e.g., 2 for ReLU)\n",
    "\n",
    "eval_metrics, duration = RVFL_train(train_data, test_data, C, N, activation)\n",
    "\n",
    "print(\"Evaluation Metrics (Accuracy, Sensitivity, Specificity, Precision, Recall, F1 Score, G-Mean):\")\n",
    "print(eval_metrics)\n",
    "print(\"Training and Testing Time:\", duration, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
