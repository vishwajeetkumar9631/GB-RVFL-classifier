{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split ,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat(r'C:\\Users\\vishw\\Documents\\karafeature\\MM08.mat')\n",
    "lab =loadmat(r'C:\\Users\\vishw\\Documents\\karafeature\\LabelMM081.mat')\n",
    "# Extract features and labels\n",
    "features = data['all_features']\n",
    "labels = lab['all_labels'].flatten()\n",
    "# labels = lab['all_labels'].flatten()  # Flatten if labels are in (8184, 1) shape\n",
    "# If labels start from 1, adjust them to start from 0\n",
    "if np.min(labels) == 1:\n",
    "    labels -= 1\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "# import sklearn.cluster.k_means_ as KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from collections import Counter\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # ignore warning\n",
    "\n",
    "\n",
    "class GranularBall:\n",
    "    \"\"\"class of the granular ball\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        :param data:  Labeled data set, the \"-2\" column is the class label, the last column is the index of each line\n",
    "        and each of the preceding columns corresponds to a feature\n",
    "        \"\"\"\n",
    "        self.data = data[:, :]\n",
    "        self.data_no_label = data[:, :-2]\n",
    "        self.num, self.dim = self.data_no_label.shape\n",
    "        self.center = self.data_no_label.mean(0)\n",
    "        self.label, self.purity ,self.r= self.__get_label_and_purity_and_r()\n",
    "\n",
    "    def __get_label_and_purity_and_r(self):\n",
    "        \"\"\"\n",
    "        :return: the label and purity of the granular ball.\n",
    "        \"\"\"\n",
    "        count = Counter(self.data[:, -2])\n",
    "        label = max(count, key=count.get)\n",
    "        purity = count[label] / self.num\n",
    "        arr=np.array(self.data_no_label)-self.center\n",
    "        ar=np.square(arr)\n",
    "        a=np.sqrt(np.sum(ar,1))\n",
    "        r=np.sum(a)/len(self.data_no_label)\n",
    "        return label, purity,r\n",
    "\n",
    "    def split_2balls(self):\n",
    "        \"\"\"\n",
    "        split the granular ball to 2 new balls by using 2_means.\n",
    "        \"\"\"\n",
    "        # label_cluster = KMeans(X=self.data_no_label, n_clusters=2)[1]\n",
    "        clu=KMeans(n_clusters=2).fit(self.data_no_label)\n",
    "        label_cluster=clu.labels_\n",
    "        if sum(label_cluster == 0) and sum(label_cluster == 1):\n",
    "            ball1 = GranularBall(self.data[label_cluster == 0, :])\n",
    "            ball2 = GranularBall(self.data[label_cluster == 1, :])\n",
    "        else:\n",
    "            ball1 = GranularBall(self.data[0:1, :]) \n",
    "            ball2 = GranularBall(self.data[1:, :])\n",
    "        return ball1, ball2\n",
    "\n",
    "\n",
    "class GBList:\n",
    "    \"\"\"class of the list of granular ball\"\"\"\n",
    "    def __init__(self, data=None):\n",
    "        self.data = data[:, :]\n",
    "        self.granular_balls = [GranularBall(self.data)]  # gbs is initialized with all data\n",
    "\n",
    "    def init_granular_balls(self, purity=1, min_sample=1):\n",
    "        \"\"\"\n",
    "        Split the balls, initialize the balls list.\n",
    "        :param purity: If the purity of a ball is greater than this value, stop splitting.\n",
    "        :param min_sample: If the number of samples of a ball is less than this value, stop splitting.\n",
    "        \"\"\"\n",
    "        ll = len(self.granular_balls)\n",
    "        i = 0\n",
    "        while True:\n",
    "            if self.granular_balls[i].purity < purity and self.granular_balls[i].num > min_sample:\n",
    "                split_balls = self.granular_balls[i].split_2balls()\n",
    "                self.granular_balls[i] = split_balls[0]\n",
    "                self.granular_balls.append(split_balls[1])\n",
    "                ll += 1\n",
    "            else:\n",
    "                i += 1\n",
    "            if i >= ll:\n",
    "                break\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data_size(self):\n",
    "        return list(map(lambda x: len(x.data), self.granular_balls))\n",
    "\n",
    "    def get_purity(self):\n",
    "        return list(map(lambda x: x.purity, self.granular_balls))\n",
    "\n",
    "    def get_center(self):\n",
    "        \"\"\"\n",
    "        :return: the center of each ball.\n",
    "        \"\"\"\n",
    "        return np.array(list(map(lambda x: x.center, self.granular_balls)))\n",
    "\n",
    "    def get_r(self):\n",
    "        \"\"\"\n",
    "        :return: 返回半径r\n",
    "        \"\"\"\n",
    "        return np.array(list(map(lambda x: x.r, self.granular_balls)))\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        :return: Data from all existing granular balls in the GBlist.\n",
    "        \"\"\"\n",
    "        list_data = [ball.data for ball in self.granular_balls]\n",
    "        return np.vstack(list_data)\n",
    "    def del_ball(self,purty=0.,num_data=0):\n",
    "        T_ball=[]\n",
    "        for ball in self.granular_balls:\n",
    "            if ball.purity>=purty and ball.num>=num_data:\n",
    "                T_ball.append(ball)\n",
    "        self.granular_balls=T_ball.copy()\n",
    "        self.data=self.get_data()\n",
    "    def re_division(self, i):\n",
    "        \"\"\"\n",
    "        Data division with the center of the ball.\n",
    "        :return: a list of new granular balls after divisions.\n",
    "        \"\"\"\n",
    "        k = len(self.granular_balls)\n",
    "        attributes = list(range(self.data.shape[1] - 2))\n",
    "        attributes.remove(i)\n",
    "        clu = KMeans(n_clusters=k, init=self.get_center()[:, attributes], max_iter=1).fit(self.data[:, attributes])\n",
    "        label_cluster = clu.labels_\n",
    "        # label_cluster = KMeans(X=self.data[:, attributes], n_clusters=k,\n",
    "        #                         init=self.get_center()[:, attributes], max_iter=1)[1]\n",
    "        granular_balls_division = []\n",
    "        for i in set(label_cluster):\n",
    "            granular_balls_division.append(GranularBall(self.data[label_cluster == i, :]))\n",
    "        return granular_balls_division\n",
    "def generate_ball_data(data,pur,delbals):\n",
    "    num, dim = data[:, :-1].shape\n",
    "    index = np.array(range(num)).reshape(num, 1)  # column of index\n",
    "    data = np.hstack((data, index))  # Add the index column to the last column of the data\n",
    "    # step 1.\n",
    "    #print(data[0:4])\n",
    "    gb = GBList(data)  # create the list of granular balls\n",
    "    gb.init_granular_balls(purity=pur)  # initialize the list\n",
    "    gb.del_ball(num_data=delbals)\n",
    "    centers=gb.get_center().tolist()\n",
    "    rs=gb.get_r().tolist()\n",
    "    # print(type(centers[0]))\n",
    "    balldata = []  # 检验\n",
    "    for i in range(len(gb.granular_balls)):\n",
    "        a=[]\n",
    "        a.append(centers[i])\n",
    "        a.append(rs[i])\n",
    "        # print(data[i][-2])\n",
    "        if gb.granular_balls[i].label==-1:\n",
    "            a.append(-1)\n",
    "        elif gb.granular_balls[i].label==1:\n",
    "            a.append(1)\n",
    "        balldata.append(a)\n",
    "    # print(balldata[0])\n",
    "    return balldata\n",
    "def gen_balls(data,pur,delbals):\n",
    "    # df=pd.read_csv(url,header=None)\n",
    "    # data=df.values\n",
    "    # print(data.shape)\n",
    "    balls=generate_ball_data(data,pur=pur,delbals=delbals)\n",
    "    R_balls=[]\n",
    "    for i in balls:\n",
    "        t_ball=[]\n",
    "        t_ball.append(i[0])\n",
    "        t_ball.append(i[1])\n",
    "        t_ball.append(i[2])\n",
    "        R_balls.append(t_ball)\n",
    "    return R_balls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optimized activation functions using NumPy\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772\n",
    "    scale = 1.0507009873554805\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.where(x > 0, x, 0.1 * x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def hardlim(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tribas(x):\n",
    "    return np.maximum(1 - np.abs(x), 0)\n",
    "\n",
    "def radbas(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "# Mapping activation functions to their respective functions\n",
    "activation_functions = {\n",
    "    1: selu,\n",
    "    2: relu,\n",
    "    3: sigmoid,\n",
    "    4: np.sin,\n",
    "    5: hardlim,\n",
    "    6: tribas,\n",
    "    7: radbas,\n",
    "    8: np.sign,\n",
    "    9: leaky_relu,\n",
    "    10: np.tanh\n",
    "}\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot(x, n_class):\n",
    "    y = np.zeros((len(x), n_class))\n",
    "    for i in range(n_class):\n",
    "        y[x == i, i] = 1\n",
    "    return y\n",
    "\n",
    "# Evaluation metrics\n",
    "def Evaluate(ACTUAL, PREDICTED):\n",
    "    p = np.sum(ACTUAL == 1)\n",
    "    n = np.sum(ACTUAL == 0)\n",
    "    \n",
    "    tp = np.sum((ACTUAL == 1) & (PREDICTED == 1))\n",
    "    tn = np.sum((ACTUAL == 0) & (PREDICTED == 0))\n",
    "    fp = np.sum((ACTUAL == 0) & (PREDICTED == 1))\n",
    "    fn = np.sum((ACTUAL == 1) & (PREDICTED == 0))\n",
    "    \n",
    "    accuracy = 100 * (tp + tn) / (p + n)\n",
    "    sensitivity = 100 * tp / p if p != 0 else 0\n",
    "    specificity = 100 * tn / n if n != 0 else 0\n",
    "    precision = 100 * tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = sensitivity\n",
    "    f_measure = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    gmean = 100 * np.sqrt((tp / p) * (tn / n)) if p != 0 and n != 0 else 0\n",
    "    \n",
    "    return [accuracy, sensitivity, specificity, precision, recall, f_measure, gmean, tp, tn, fp, fn]\n",
    "\n",
    "def RVFL_train(train_data, test_data, C, N, activation):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Split training data\n",
    "    trainX = train_data[:, :-1]\n",
    "    trainY = train_data[:, -1]\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    nclass = 2\n",
    "    trainY_encoded = one_hot(trainY.astype(int), nclass)\n",
    "    \n",
    "    # Initialize random weights and biases\n",
    "    Nsample, Nfea = trainX.shape\n",
    "    W = np.random.uniform(-1, 1, (Nfea, N))\n",
    "    b = 0.1 * np.random.rand(1, N)\n",
    "    \n",
    "    # Compute hidden layer output for training data\n",
    "    X1 = np.dot(trainX, W) + b\n",
    "    activation_func = activation_functions.get(activation, relu)\n",
    "    X1 = activation_func(X1)\n",
    "    \n",
    "    # Augment the input with hidden layer output and bias term\n",
    "    X = np.hstack((trainX, X1, np.ones((Nsample, 1))))\n",
    "    \n",
    "    # Compute output weights (beta) using regularized least squares\n",
    "    if X.shape[1] < Nsample:\n",
    "        beta = np.linalg.inv(np.eye(X.shape[1]) / C + X.T @ X) @ X.T @ trainY_encoded\n",
    "    else:\n",
    "        beta = X.T @ np.linalg.inv(np.eye(X.shape[0]) / C + X @ X.T) @ trainY_encoded\n",
    "\n",
    "    # Test phase\n",
    "    testX = test_data[:, :-1]\n",
    "    testY = test_data[:, -1]\n",
    "    \n",
    "    X1_test = np.dot(testX, W) + b\n",
    "    X1_test = activation_func(X1_test)\n",
    "    \n",
    "    # Augment test data\n",
    "    X_test = np.hstack((testX, X1_test, np.ones((testX.shape[0], 1))))\n",
    "    \n",
    "    # Calculate scores and classify\n",
    "    raw_scores = X_test @ beta\n",
    "    predicted_labels = np.argmax(raw_scores, axis=1)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    eval_metrics = Evaluate(testY, predicted_labels)\n",
    "    end = time.time()\n",
    "    \n",
    "    return eval_metrics, end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m pur \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.015\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     31\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 32\u001b[0m A_train \u001b[38;5;241m=\u001b[39m \u001b[43mgen_balls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAA_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpur\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelbals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract Centers, Radii, and Labels\u001b[39;00m\n\u001b[0;32m     35\u001b[0m Centers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m A_train])\n",
      "Cell \u001b[1;32mIn[4], line 170\u001b[0m, in \u001b[0;36mgen_balls\u001b[1;34m(data, pur, delbals)\u001b[0m\n\u001b[0;32m    168\u001b[0m     t_ball\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m     t_ball\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 170\u001b[0m     t_ball\u001b[38;5;241m.\u001b[39mappend(\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    171\u001b[0m     R_balls\u001b[38;5;241m.\u001b[39mappend(t_ball)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m R_balls\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# from gen_ball import gen_balls\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'features' and 'labels' are already loaded\n",
    "# Ensure labels are in the right shape\n",
    "# labels = labels.flatten()\n",
    "\n",
    "# Combine features and labels into one dataset\n",
    "dataset = np.hstack((features, labels.reshape(-1, 1)))\n",
    "\n",
    "# Convert label 0 to -1 for compatibility with gen_balls\n",
    "dataset[dataset[:, -1] == 0, -1] = -1\n",
    "\n",
    "# Shuffle the dataset\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "AA_train = dataset[:train_size]\n",
    "A_test = dataset[train_size:]\n",
    "\n",
    "# Convert -1 labels back to 0 for the test set\n",
    "A_test[A_test[:, -1] == -1, -1] = 0\n",
    "\n",
    "# Generate synthetic balls for training data\n",
    "pur = 1 - (0.015 * 5)\n",
    "num = 2\n",
    "A_train = gen_balls(AA_train, pur=pur, delbals=num)\n",
    "\n",
    "# Extract Centers, Radii, and Labels\n",
    "Centers = np.array([item[0] for item in A_train])\n",
    "Labels = np.array([item[-1] for item in A_train]).reshape(-1, 1)\n",
    "\n",
    "# Prepare the final training data\n",
    "A_train = np.hstack((Centers, Labels))\n",
    "\n",
    "# Convert -1 labels back to 0 for the training set\n",
    "A_train[A_train[:, -1] == -1, -1] = 0\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "A_train[:, :-1] = scaler.fit_transform(A_train[:, :-1])\n",
    "A_test[:, :-1] = scaler.transform(A_test[:, :-1])\n",
    "\n",
    "# RVFL parameters (these can be tuned further)\n",
    "c1_best = 0.01\n",
    "N_best = 203\n",
    "Act_best = 8\n",
    "\n",
    "# Implementing K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(A_train):\n",
    "    fold_train, fold_val = A_train[train_index], A_train[val_index]\n",
    "    \n",
    "    # Train the RVFL model on the fold\n",
    "    Eval, Test_time = RVFL_train(fold_train, A_test, c1_best, N_best, Act_best)\n",
    "    \n",
    "    # Capture the test accuracy\n",
    "    Test_accuracy = Eval[0]\n",
    "    accuracy_scores.append(Test_accuracy)\n",
    "    print(f\"Fold Test Accuracy: {Test_accuracy:.2f}%\")\n",
    "\n",
    "# Average accuracy across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "print(f\"\\nAverage Test Accuracy: {avg_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
